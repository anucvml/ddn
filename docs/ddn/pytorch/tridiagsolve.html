<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>ddn.pytorch.tridiagsolve API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ddn.pytorch.tridiagsolve</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># DIFFERENTIABLE TRIDIAGONAL MATRIX SOLVER
# Differentiable solver for batches of tridiagonal linear systems.
#
# Stephen Gould &lt;stephen.gould@anu.edu.au&gt;
#

import torch

def tridiagsolve(b, a, c, d, method=&#39;cyclic&#39;):
    &#34;&#34;&#34;
    Batch tridiagonal matrix algorithm based on either cyclic reduction (method=&#39;cyclic&#39;) or the Thomas algorithm
    (method=&#39;thomas&#39;) to solve systems of the form,

        | b_1 c_1     ...  0  | | x_1 |   | d_1 |
        | a_2 b_2 c_2 ...  0  | | x_2 |   | d_2 |
        |        .            | |     |   |  .  |
        |           .         | |     | = |  .  |
        |              .      | |     |   |  .  |
        |             ... b_n | | x_n |   | d_n |

    The Thomas algorithm is an efficient implementation of Gaussian elimination and good for single-threaded hardware
    or on large batches of data. The input matrix needs to be diagonally dominant or positive semi-definite for stable
    behaviour.
    Cyclic reduction recursively eliminates odd and even variables to produce two independent problems. It is good for
    multi-threaded hardware or small batches with large sequence lengths.

    :param b: main diagonal, (b_1, ..., b_n), of size (B x N)
    :param a: lower diagonal, (a_2, ..., a_n), of size (B x N-1)
    :param c: upper diagonal, (c_1, ..., c_{n-1}), of size (B x N-1)
    :param d: right-hand-size, (d_1, ..., d_n), of size (B x N x M)
    :param method: &#39;cyclic&#39; or &#39;thomas&#39;
    :return: x = (x_1, ..., x_n) of size (B x N x M)
    &#34;&#34;&#34;

    assert len(d.shape) == 3, &#34;argument &#39;d&#39; must have shape (B, N, M) or (B, N, 1)&#34;
    B, N, M = d.shape
    assert b.shape == (B, N), &#34;argument &#39;b&#39; must have shape (B, N)&#34;
    assert a.shape == (B, N-1), &#34;argument &#39;a&#39; must have shape (B, N-1)&#34;
    assert c.shape == (B, N-1), &#34;argument &#39;c&#39; must have shape (B, N-1)&#34;

    if method == &#39;cyclic&#39;:
        # initialize
        a_dash, c_dash = a, c
        b_dash, d_dash = b.clone(), d.clone()

        # repeat until problems of size 1
        h = 1
        while (h &lt; N):
            # eliminate odd/even terms
            alpha = -1.0 * a_dash / b_dash[:, :-h]      # i = h/2+2, ..., n-h/2
            beta = -1.0 * c_dash / b_dash[:, h:]        # i = h/2+1, ..., n-h/2-1

            b_dash[:, h:] += alpha * c_dash             # i = h/2+1, ..., n-h/2
            b_dash[:, :-h] += beta * a_dash             # i = h/2+2, ..., n-h/2-1
            d_prev = d_dash.clone()                     # i = h/2+1, ..., n-h/2
            d_dash[:, h:, :] += alpha.view(B, N-h, 1) * d_prev[:, :-h, :]    # i = h/2+2, ..., n-h/2
            d_dash[:, :-h, :] += beta.view(B, N-h, 1) * d_prev[:, h:, :]     # i = h/2+1, ..., n-h/2-1

            if (h &lt; alpha.shape[1]):
                a_dash = alpha[:, h:] * a_dash[:, :-h]  # i = h/2+1, ..., n-h/2
                c_dash = beta[:, :-h] * c_dash[:, h:]   # i = h/2+1, ..., n-h/2

            h *= 2

        # solve
        return d_dash / b_dash.view(B, N, 1)

    elif method == &#39;thomas&#39;:

        # initialize
        x = torch.empty_like(d)
        c_dash = torch.empty_like(c)
        d_dash = torch.empty_like(d)

        # forward elimination
        c_dash[:, 0] = c[:, 0] / b[:, 0]
        d_dash[:, 0, :] = d[:, 0, :] / b[:, 0].view(B, 1)

        for i in range(1, N-1):
            w = b[:, i] - a[:, i-1] * c_dash[:, i-1]
            c_dash[:, i] = c[:, i] / w
            d_dash[:, i, :] = (d[:, i, :] - a[:, i-1].view(B, 1) * d_dash[:, i-1, :]) / w.view(B, 1)

        w = b[:, N-1] - a[:, N-2] * c_dash[:, N-2]
        d_dash[:, N-1, :] = (d[:, N-1, :] - a[:, N-2].view(B, 1) * d_dash[:, N-2, :]) / w.view(B, 1)

        # backward substitution
        x[:, N-1, :] = d_dash[:, N-1, :]
        for i in range(N-1, 0, -1):
            x[:, i-1, :] = d_dash[:, i-1, :] - c_dash[:, i-1].view(B, 1) * x[:, i, :]

        return x

    else:
        raise NameError(&#34;unknown method &#39;{}&#39;&#34;.format(method))


class TriDiagSolveFcn(torch.autograd.Function):
    &#34;&#34;&#34;
    Differentiable tridiagonal matrix solver. See `tridiagsolve`.
    &#34;&#34;&#34;

    @staticmethod
    def forward(ctx, b, a, c, d, method=&#39;cyclic&#39;):
        with torch.no_grad():
            x = tridiagsolve(b, a, c, d, method)
        ctx.save_for_backward(b, a, c, d, x)
        ctx.method = method
        return x

    @staticmethod
    def backward(ctx, grad_x):
        b, a, c, d, x = ctx.saved_tensors

        w = tridiagsolve(b, c, a, grad_x, ctx.method)
        grad_b = -1.0 * torch.sum(w * x, 2) if ctx.needs_input_grad[0] else None
        grad_a = -1.0 * torch.sum(w[:, 1:] * x[:, :-1], 2) if ctx.needs_input_grad[1] else None
        grad_c = -1.0 * torch.sum(w[:, :-1] * x[:, 1:], 2) if ctx.needs_input_grad[2] else None
        grad_d = w if ctx.needs_input_grad[3] else None

        return grad_b, grad_a, grad_c, grad_d, None


#
# --- testing ---
#

if __name__ == &#39;__main__&#39;:

    B, N, M = 2, 16, 5
    type = torch.float64
    device = torch.device(&#34;cpu&#34;)

    # arbitrary
    b = 2.0 * torch.ones((B, N), dtype=type, device=device) + 0.1 * torch.rand((B, N), dtype=type, device=device, requires_grad=True)
    a = -1.0 * torch.rand((B, N - 1), dtype=type, device=device, requires_grad=True)
    c = -1.0 * torch.rand((B, N - 1), dtype=type, device=device, requires_grad=True)
    d = torch.rand((B, N, M), dtype=type, device=device, requires_grad=True)

    print(&#34;Checking implementation accuracy on arbitrary input...&#34;)
    print(d.shape)
    A = torch.diag_embed(b) + torch.diag_embed(a, offset=-1) + torch.diag_embed(c, offset=1)

    x = tridiagsolve(b, a, c, d, &#39;cyclic&#39;)
    print(torch.max(torch.abs(A @ x - d)).item())

    x = tridiagsolve(b, a, c, d, &#39;thomas&#39;)
    print(torch.max(torch.abs(A @ x - d)).item())

    # poisson
    print(&#34;Checking implementation accuracy on poisson input...&#34;)
    neg_ones = -1.0 * torch.ones((B, N-1), dtype=type, device=device)
    A = torch.diag_embed(b) + torch.diag_embed(neg_ones, offset=-1) + torch.diag_embed(neg_ones, offset=1)

    x = tridiagsolve(b, neg_ones, neg_ones, d, &#39;cyclic&#39;)
    print(torch.max(torch.abs(A @ x - d)).item())

    x = tridiagsolve(b, neg_ones, neg_ones, d, &#39;thomas&#39;)
    print(torch.max(torch.abs(A @ x - d)).item())

    #exit(0)

    from torch.autograd import gradcheck

    torch.manual_seed(22)
    device = torch.device(&#34;cuda&#34;) if torch.cuda.is_available() else torch.device(&#34;cpu&#34;)
    #device = torch.device(&#34;cpu&#34;)

    B, N, M = 2, 16, 5
    type = torch.float64
    
    b = 2.0 * torch.ones((B, N), dtype=type, device=device) + 0.1 * torch.rand((B, N), dtype=type, device=device, requires_grad=True)
    a = -1.0 * torch.rand((B, N-1), dtype=type, device=device, requires_grad=True)
    c = -1.0 * torch.rand((B, N-1), dtype=type, device=device, requires_grad=True)
    d = torch.rand((B, N, M), dtype=type, device=device, requires_grad=True)

    print(&#34;Checking gradients (cyclic)...&#34;)
    test = gradcheck(TriDiagSolveFcn().apply, (b, a, c, d, &#39;cyclic&#39;), eps=1e-6, atol=1e-3, rtol=1e-6)
    print(test)

    print(&#34;Checking gradients (thomas)...&#34;)
    test = gradcheck(TriDiagSolveFcn().apply, (b, a, c, d, &#39;thomas&#39;), eps=1e-6, atol=1e-3, rtol=1e-6)
    print(test)

    #exit(0)

    import time
    print(&#34;Testing running time...&#34;)

    N = 256
    type = torch.float32

    for B in (1, 10, 100, 1000):
        b = 2.0 * torch.ones((B, N), dtype=type, device=device) + 0.1 * torch.rand((B, N), dtype=type, device=device, requires_grad=True)
        #a = -1.0 * torch.ones((B, N-1), dtype=type, device=device, requires_grad=True)
        #c = -1.0 * torch.ones((B, N-1), dtype=type, device=device, requires_grad=True)
        a = -1.0 * torch.rand((B, N - 1), dtype=type, device=device, requires_grad=True)
        c = -1.0 * torch.rand((B, N - 1), dtype=type, device=device, requires_grad=True)
        d = torch.rand((B, N, 1), dtype=type, device=device, requires_grad=True)

        print(&#34;...data size {}&#34;.format(d.shape))

        start = time.time()
        x = TriDiagSolveFcn.apply(b.clone().detach().requires_grad_(True), a.clone().detach().requires_grad_(True),
                                  c.clone().detach().requires_grad_(True), d.clone().detach().requires_grad_(True), &#39;cyclic&#39;)
        x_elapsed = time.time() - start

        start = time.time()
        x.sum().backward()
        dx_elapsed = time.time() - start

        start = time.time()
        y = TriDiagSolveFcn.apply(b.clone().detach().requires_grad_(True), a.clone().detach().requires_grad_(True),
                                  c.clone().detach().requires_grad_(True), d.clone().detach().requires_grad_(True), &#39;thomas&#39;)
        y_elapsed = time.time() - start

        start = time.time()
        y.sum().backward()
        dy_elapsed = time.time() - start

        A = torch.diag_embed(b.clone().detach().requires_grad_(True)) + \
            torch.diag_embed(a.clone().detach().requires_grad_(True), offset=-1) + \
            torch.diag_embed(c.clone().detach().requires_grad_(True), offset=1)
        start = time.time()
        z = torch.linalg.solve(A, d)
        z_elapsed = time.time() - start
        print(z.shape)

        start = time.time()
        z.sum().backward()
        dz_elapsed = time.time() - start

        print(&#39;accuracy cyclic/thomas/linalg.solve: {:.3e}/{:.3e}/{:.3e}&#39;.format(torch.max(torch.abs(A @ x - d)).item(),
            torch.max(torch.abs(A @ y - d)).item(), torch.max(torch.abs(A @ z - d)).item()))
        print(&#39; forward cyclic/thomas/linalg.solve: {:.3e}/{:.3e}/{:.3e}&#39;.format(x_elapsed, y_elapsed, z_elapsed))
        print(&#39;backward cyclic/thomas/linalg.solve: {:.3e}/{:.3e}/{:.3e}&#39;.format(dx_elapsed, dy_elapsed, dz_elapsed))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ddn.pytorch.tridiagsolve.tridiagsolve"><code class="name flex">
<span>def <span class="ident">tridiagsolve</span></span>(<span>b, a, c, d, method='cyclic')</span>
</code></dt>
<dd>
<div class="desc"><p>Batch tridiagonal matrix algorithm based on either cyclic reduction (method='cyclic') or the Thomas algorithm
(method='thomas') to solve systems of the form,</p>
<pre><code>| b_1 c_1     ...  0  | | x_1 |   | d_1 |
| a_2 b_2 c_2 ...  0  | | x_2 |   | d_2 |
|        .            | |     |   |  .  |
|           .         | |     | = |  .  |
|              .      | |     |   |  .  |
|             ... b_n | | x_n |   | d_n |
</code></pre>
<p>The Thomas algorithm is an efficient implementation of Gaussian elimination and good for single-threaded hardware
or on large batches of data. The input matrix needs to be diagonally dominant or positive semi-definite for stable
behaviour.
Cyclic reduction recursively eliminates odd and even variables to produce two independent problems. It is good for
multi-threaded hardware or small batches with large sequence lengths.</p>
<p>:param b: main diagonal, (b_1, &hellip;, b_n), of size (B x N)
:param a: lower diagonal, (a_2, &hellip;, a_n), of size (B x N-1)
:param c: upper diagonal, (c_1, &hellip;, c_{n-1}), of size (B x N-1)
:param d: right-hand-size, (d_1, &hellip;, d_n), of size (B x N x M)
:param method: 'cyclic' or 'thomas'
:return: x = (x_1, &hellip;, x_n) of size (B x N x M)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tridiagsolve(b, a, c, d, method=&#39;cyclic&#39;):
    &#34;&#34;&#34;
    Batch tridiagonal matrix algorithm based on either cyclic reduction (method=&#39;cyclic&#39;) or the Thomas algorithm
    (method=&#39;thomas&#39;) to solve systems of the form,

        | b_1 c_1     ...  0  | | x_1 |   | d_1 |
        | a_2 b_2 c_2 ...  0  | | x_2 |   | d_2 |
        |        .            | |     |   |  .  |
        |           .         | |     | = |  .  |
        |              .      | |     |   |  .  |
        |             ... b_n | | x_n |   | d_n |

    The Thomas algorithm is an efficient implementation of Gaussian elimination and good for single-threaded hardware
    or on large batches of data. The input matrix needs to be diagonally dominant or positive semi-definite for stable
    behaviour.
    Cyclic reduction recursively eliminates odd and even variables to produce two independent problems. It is good for
    multi-threaded hardware or small batches with large sequence lengths.

    :param b: main diagonal, (b_1, ..., b_n), of size (B x N)
    :param a: lower diagonal, (a_2, ..., a_n), of size (B x N-1)
    :param c: upper diagonal, (c_1, ..., c_{n-1}), of size (B x N-1)
    :param d: right-hand-size, (d_1, ..., d_n), of size (B x N x M)
    :param method: &#39;cyclic&#39; or &#39;thomas&#39;
    :return: x = (x_1, ..., x_n) of size (B x N x M)
    &#34;&#34;&#34;

    assert len(d.shape) == 3, &#34;argument &#39;d&#39; must have shape (B, N, M) or (B, N, 1)&#34;
    B, N, M = d.shape
    assert b.shape == (B, N), &#34;argument &#39;b&#39; must have shape (B, N)&#34;
    assert a.shape == (B, N-1), &#34;argument &#39;a&#39; must have shape (B, N-1)&#34;
    assert c.shape == (B, N-1), &#34;argument &#39;c&#39; must have shape (B, N-1)&#34;

    if method == &#39;cyclic&#39;:
        # initialize
        a_dash, c_dash = a, c
        b_dash, d_dash = b.clone(), d.clone()

        # repeat until problems of size 1
        h = 1
        while (h &lt; N):
            # eliminate odd/even terms
            alpha = -1.0 * a_dash / b_dash[:, :-h]      # i = h/2+2, ..., n-h/2
            beta = -1.0 * c_dash / b_dash[:, h:]        # i = h/2+1, ..., n-h/2-1

            b_dash[:, h:] += alpha * c_dash             # i = h/2+1, ..., n-h/2
            b_dash[:, :-h] += beta * a_dash             # i = h/2+2, ..., n-h/2-1
            d_prev = d_dash.clone()                     # i = h/2+1, ..., n-h/2
            d_dash[:, h:, :] += alpha.view(B, N-h, 1) * d_prev[:, :-h, :]    # i = h/2+2, ..., n-h/2
            d_dash[:, :-h, :] += beta.view(B, N-h, 1) * d_prev[:, h:, :]     # i = h/2+1, ..., n-h/2-1

            if (h &lt; alpha.shape[1]):
                a_dash = alpha[:, h:] * a_dash[:, :-h]  # i = h/2+1, ..., n-h/2
                c_dash = beta[:, :-h] * c_dash[:, h:]   # i = h/2+1, ..., n-h/2

            h *= 2

        # solve
        return d_dash / b_dash.view(B, N, 1)

    elif method == &#39;thomas&#39;:

        # initialize
        x = torch.empty_like(d)
        c_dash = torch.empty_like(c)
        d_dash = torch.empty_like(d)

        # forward elimination
        c_dash[:, 0] = c[:, 0] / b[:, 0]
        d_dash[:, 0, :] = d[:, 0, :] / b[:, 0].view(B, 1)

        for i in range(1, N-1):
            w = b[:, i] - a[:, i-1] * c_dash[:, i-1]
            c_dash[:, i] = c[:, i] / w
            d_dash[:, i, :] = (d[:, i, :] - a[:, i-1].view(B, 1) * d_dash[:, i-1, :]) / w.view(B, 1)

        w = b[:, N-1] - a[:, N-2] * c_dash[:, N-2]
        d_dash[:, N-1, :] = (d[:, N-1, :] - a[:, N-2].view(B, 1) * d_dash[:, N-2, :]) / w.view(B, 1)

        # backward substitution
        x[:, N-1, :] = d_dash[:, N-1, :]
        for i in range(N-1, 0, -1):
            x[:, i-1, :] = d_dash[:, i-1, :] - c_dash[:, i-1].view(B, 1) * x[:, i, :]

        return x

    else:
        raise NameError(&#34;unknown method &#39;{}&#39;&#34;.format(method))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ddn.pytorch.tridiagsolve.TriDiagSolveFcn"><code class="flex name class">
<span>class <span class="ident">TriDiagSolveFcn</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Differentiable tridiagonal matrix solver. See <code><a title="ddn.pytorch.tridiagsolve.tridiagsolve" href="#ddn.pytorch.tridiagsolve.tridiagsolve">tridiagsolve()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TriDiagSolveFcn(torch.autograd.Function):
    &#34;&#34;&#34;
    Differentiable tridiagonal matrix solver. See `tridiagsolve`.
    &#34;&#34;&#34;

    @staticmethod
    def forward(ctx, b, a, c, d, method=&#39;cyclic&#39;):
        with torch.no_grad():
            x = tridiagsolve(b, a, c, d, method)
        ctx.save_for_backward(b, a, c, d, x)
        ctx.method = method
        return x

    @staticmethod
    def backward(ctx, grad_x):
        b, a, c, d, x = ctx.saved_tensors

        w = tridiagsolve(b, c, a, grad_x, ctx.method)
        grad_b = -1.0 * torch.sum(w * x, 2) if ctx.needs_input_grad[0] else None
        grad_a = -1.0 * torch.sum(w[:, 1:] * x[:, :-1], 2) if ctx.needs_input_grad[1] else None
        grad_c = -1.0 * torch.sum(w[:, :-1] * x[:, 1:], 2) if ctx.needs_input_grad[2] else None
        grad_d = w if ctx.needs_input_grad[3] else None

        return grad_b, grad_a, grad_c, grad_d, None</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.autograd.function.Function</li>
<li>torch.autograd.function._SingleLevelFunction</li>
<li>torch._C._FunctionBase</li>
<li>torch.autograd.function.FunctionCtx</li>
<li>torch.autograd.function._HookMixin</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="ddn.pytorch.tridiagsolve.TriDiagSolveFcn.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>ctx, grad_x)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines a formula for differentiating the operation with backward mode
automatic differentiation (alias to the vjp function).</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context :attr:<code>ctx</code> as the first argument, followed by
as many outputs as the :func:<code>forward</code> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
:func:<code>forward</code>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute :attr:<code>ctx.needs_input_grad</code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
:func:<code>backward</code> will have <code>ctx.needs_input_grad[0] = True</code> if the
first input to :func:<code>forward</code> needs gradient computed w.r.t. the
output.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def backward(ctx, grad_x):
    b, a, c, d, x = ctx.saved_tensors

    w = tridiagsolve(b, c, a, grad_x, ctx.method)
    grad_b = -1.0 * torch.sum(w * x, 2) if ctx.needs_input_grad[0] else None
    grad_a = -1.0 * torch.sum(w[:, 1:] * x[:, :-1], 2) if ctx.needs_input_grad[1] else None
    grad_c = -1.0 * torch.sum(w[:, :-1] * x[:, 1:], 2) if ctx.needs_input_grad[2] else None
    grad_d = w if ctx.needs_input_grad[3] else None

    return grad_b, grad_a, grad_c, grad_d, None</code></pre>
</details>
</dd>
<dt id="ddn.pytorch.tridiagsolve.TriDiagSolveFcn.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>ctx, b, a, c, d, method='cyclic')</span>
</code></dt>
<dd>
<div class="desc"><p>This function is to be overridden by all subclasses. There are two ways
to define forward:</p>
<p>Usage 1 (Combined forward and ctx)::</p>
<pre><code>@staticmethod
def forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:
    pass
</code></pre>
<ul>
<li>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</li>
<li>See :ref:<code>combining-forward-context</code> for more details</li>
</ul>
<p>Usage 2 (Separate forward and ctx)::</p>
<pre><code>@staticmethod
def forward(*args: Any, **kwargs: Any) -&gt; Any:
    pass

@staticmethod
def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:
    pass
</code></pre>
<ul>
<li>The forward no longer accepts a ctx argument.</li>
<li>Instead, you must also override the :meth:<code>torch.autograd.Function.setup_context</code>
staticmethod to handle setting up the <code>ctx</code> object.
<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs
to the forward.</li>
<li>See :ref:<code>extending-autograd</code> for more details</li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <code>ctx</code> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
:func:<code>ctx.save_for_backward</code> if they are intended to be used in
<code>backward</code> (equivalently, <code>vjp</code>) or :func:<code>ctx.save_for_forward</code>
if they are intended to be used for in <code>jvp</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def forward(ctx, b, a, c, d, method=&#39;cyclic&#39;):
    with torch.no_grad():
        x = tridiagsolve(b, a, c, d, method)
    ctx.save_for_backward(b, a, c, d, x)
    ctx.method = method
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ddn.pytorch" href="index.html">ddn.pytorch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ddn.pytorch.tridiagsolve.tridiagsolve" href="#ddn.pytorch.tridiagsolve.tridiagsolve">tridiagsolve</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ddn.pytorch.tridiagsolve.TriDiagSolveFcn" href="#ddn.pytorch.tridiagsolve.TriDiagSolveFcn">TriDiagSolveFcn</a></code></h4>
<ul class="">
<li><code><a title="ddn.pytorch.tridiagsolve.TriDiagSolveFcn.backward" href="#ddn.pytorch.tridiagsolve.TriDiagSolveFcn.backward">backward</a></code></li>
<li><code><a title="ddn.pytorch.tridiagsolve.TriDiagSolveFcn.forward" href="#ddn.pytorch.tridiagsolve.TriDiagSolveFcn.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>