<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>ddn.pytorch.robustpool API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ddn.pytorch.robustpool</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#
# Robust pooling
#
# y(x) = argmin_u f(x, u)
#
# where f(x, u) = sum_{i=1}^n phi(u - x_i; alpha)
# with penalty function phi in
# {quadratic, pseudo-huber, huber, welsch, truncated quadratic}
#
# Dylan Campbell &lt;dylan.campbell@anu.edu.au&gt;
# Stephen Gould &lt;stephen.gould@anu.edu.au&gt;
#

import torch

class Quadratic():
    is_convex = True

    @staticmethod
    def phi(z, alpha = 1.0):
        &#34;&#34;&#34; Quadratic penalty function

        phi(z; alpha) = 0.5 * z^2

        Arguments:
            z: (b, ...) Torch tensor,
                batch of residuals

            alpha: float, optional, default: 1.0,
                ignored

        Return Values:
            phi_z: (b, ...) Torch tensor,
                Quadratic penalty associated with each residual

        Complexity:
            O(1)
        &#34;&#34;&#34;
        phi_at_z = 0.5 * torch.pow(z, 2)
        return phi_at_z

    @staticmethod
    def Dy(z, alpha = 1.0):
        # Derivative of y(x) for the quadratic penalty function
        Dy_at_x = torch.ones_like(z) / (z.size(-1) * z.size(-2))
        return Dy_at_x

class PseudoHuber():
    is_convex = True

    @staticmethod
    def phi(z, alpha = 1.0):
        &#34;&#34;&#34; Pseudo-Huber penalty function

        phi(z; alpha) = alpha^2 (sqrt{1 + (z / alpha)^2} - 1)

        Arguments:
            z: (b, ...) Torch tensor,
                batch of residuals

            alpha: float, optional, default: 1.0,
                ~slope of the linear region
                ~maximum residual in the quadratic region

        Return Values:
            phi_z: (b, ...) Torch tensor,
                Pseudo-Huber penalty associated with each residual

        Complexity:
            O(1)
        &#34;&#34;&#34;
        assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
        alpha2 = alpha * alpha
        phi_at_z = alpha2 * (torch.sqrt(1.0 + torch.pow(z, 2) / alpha2) - 1.0)
        return phi_at_z

    @staticmethod
    def Dy(z, alpha = 1.0):
        # Derivative of y(x) for the pseudo-Huber penalty function
        w = torch.pow(1.0 + torch.pow(z, 2) / (alpha * alpha), -1.5)
        w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
        Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
        return Dy_at_x

class Huber():
    is_convex = True

    @staticmethod
    def phi(z, alpha = 1.0):
        &#34;&#34;&#34; Huber penalty function

                        | 0.5 z^2 for |z| &lt;= alpha
        phi(z; alpha) = |
                        | alpha (|z| - 0.5 alpha) else

        Arguments:
            z: (b, ...) Torch tensor,
                batch of residuals

            alpha: float, optional, default: 1.0,
                slope of the linear region
                maximum residual in the quadratic region

        Return Values:
            phi_z: (b, ...) Torch tensor,
                Huber penalty associated with each residual

        Complexity:
            O(1)
        &#34;&#34;&#34;
        assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
        z = z.abs()
        phi_at_z = torch.where(z &lt;= alpha, 0.5 * torch.pow(z, 2), alpha * (z - 0.5 * alpha))
        return phi_at_z

    @staticmethod
    def Dy(z, alpha = 1.0):
        # Derivative of y(x) for the Huber penalty function
        w = torch.where(z.abs() &lt;= alpha, torch.ones_like(z), torch.zeros_like(z))
        w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
        Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
        return Dy_at_x

class Welsch():
    is_convex = False

    @staticmethod
    def phi(z, alpha = 1.0):
        &#34;&#34;&#34; Welsch penalty function

        phi(z; alpha) = 1 - exp(-0.5 * z^2 / alpha^2)

        Arguments:
            z: (b, ...) Torch tensor,
                batch of residuals

            alpha: float, optional, default: 1.0,
                ~maximum residual in the quadratic region

        Return Values:
            phi_z: (b, ...) Torch tensor,
                Welsch penalty associated with each residual

        Complexity:
            O(1)
        &#34;&#34;&#34;
        assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
        phi_at_z = 1.0 - torch.exp(-torch.pow(z, 2) / (2.0 * alpha * alpha))
        return phi_at_z

    @staticmethod
    def Dy(z, alpha = 1.0):
        # Derivative of y(x) for the Welsch penalty function
        alpha2 = alpha * alpha
        z2_on_alpha2 = torch.pow(z, 2) / alpha2
        w = (1.0 - z2_on_alpha2) * torch.exp(-0.5 * z2_on_alpha2) / alpha2
        w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
        Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
        Dy_at_x = torch.clamp(Dy_at_x, -1.0, 1.0) # Clip gradients to +/- 1
        return Dy_at_x

class TruncatedQuadratic():
    is_convex = False

    @staticmethod
    def phi(z, alpha = 1.0):
        &#34;&#34;&#34; Truncated quadratic penalty function

                        | 0.5 z^2 for |z| &lt;= alpha
        phi(z; alpha) = |
                        | 0.5 alpha^2 else

        Arguments:
            z: (b, ...) Torch tensor,
                batch of residuals

            alpha: float, optional, default: 1.0,
                maximum residual in the quadratic region

        Return Values:
            phi_z: (b, ...) Torch tensor,
                Truncated quadratic penalty associated with each residual

        Complexity:
            O(1)
        &#34;&#34;&#34;
        assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
        z = z.abs()
        phi_at_z = torch.where(z &lt;= alpha, 0.5 * torch.pow(z, 2), 0.5 * alpha * alpha * torch.ones_like(z))
        return phi_at_z

    @staticmethod
    def Dy(z, alpha = 1.0):
        # Derivative of y(x) for the truncated quadratic penalty function
        w = torch.where(z.abs() &lt;= alpha, torch.ones_like(z), torch.zeros_like(z))
        w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
        Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
        return Dy_at_x

class RobustGlobalPool2dFn(torch.autograd.Function):
    &#34;&#34;&#34;
    A function to globally pool a 2D response matrix using a robust penalty function
    &#34;&#34;&#34;
    @staticmethod
    def runOptimisation(x, y, method, alpha_scalar):
        with torch.enable_grad():
            opt = torch.optim.LBFGS([y],
                                    lr=1, # Default: 1
                                    max_iter=100, # Default: 20
                                    max_eval=None, # Default: None
                                    tolerance_grad=1e-05, # Default: 1e-05
                                    tolerance_change=1e-09, # Default: 1e-09
                                    history_size=100, # Default: 100
                                    line_search_fn=None # Default: None, Alternative: &#34;strong_wolfe&#34;
                                    )
            def reevaluate():
                opt.zero_grad()
                # Sum cost function across residuals and batch (all fi are positive)
                f = method.phi(y.unsqueeze(-1).unsqueeze(-1) - x, alpha=alpha_scalar).sum()
                f.backward()
                return f
            opt.step(reevaluate)
        return y

    @staticmethod
    def forward(ctx, x, method, alpha):
        input_size = x.size()
        assert len(input_size) &gt;= 2, &#34;input must at least 2D (%d &lt; 2)&#34; % len(input_size)
        alpha_scalar = alpha.item()
        assert alpha.item() &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha.item()
        x = x.detach()
        x = x.flatten(end_dim=-3) if len(input_size) &gt; 2 else x
        # Handle non-convex functions separately
        if method.is_convex:
            # Use mean as initial guess
            y = x.mean([-2, -1]).clone().requires_grad_()
            y = RobustGlobalPool2dFn.runOptimisation(x, y, method, alpha_scalar)
        else:
            # Use mean and median as initial guesses and choose the best
            # ToDo: multiple random starts
            y_mean = x.mean([-2, -1]).clone().requires_grad_()
            y_mean = RobustGlobalPool2dFn.runOptimisation(x, y_mean, method, alpha_scalar)
            y_median = x.flatten(start_dim=-2).median(dim=-1)[0].clone().requires_grad_()
            y_median = RobustGlobalPool2dFn.runOptimisation(x, y_median, method, alpha_scalar)
            f_mean = method.phi(y_mean.unsqueeze(-1).unsqueeze(-1) - x, alpha=alpha_scalar).sum(-1).sum(-1)
            f_median = method.phi(y_median.unsqueeze(-1).unsqueeze(-1) - x, alpha=alpha_scalar).sum(-1).sum(-1)
            y = torch.where(f_mean &lt;= f_median, y_mean, y_median)
        y = y.detach()
        z = (y.unsqueeze(-1).unsqueeze(-1) - x).clone()
        ctx.method = method
        ctx.input_size = input_size
        ctx.save_for_backward(z, alpha)
        return y.reshape(input_size[:-2]).clone()

    @staticmethod
    def backward(ctx, grad_output):
        z, alpha = ctx.saved_tensors
        input_size = ctx.input_size
        method = ctx.method
        grad_input = None
        if ctx.needs_input_grad[0]:
            # Flatten:
            grad_output = grad_output.detach().flatten(end_dim=-1)
            # Use implicit differentiation to compute derivative:
            grad_input = method.Dy(z, alpha) * grad_output.unsqueeze(-1).unsqueeze(-1)
            # Unflatten:
            grad_input = grad_input.reshape(input_size)
        return grad_input, None, None

class RobustGlobalPool2d(torch.nn.Module):
    def __init__(self, method, alpha=1.0):
        super(RobustGlobalPool2d, self).__init__()
        self.method = method
        self.register_buffer(&#39;alpha&#39;, torch.tensor([alpha]))

    def forward(self, input):
        return RobustGlobalPool2dFn.apply(input,
                                          self.method,
                                          self.alpha
                                          )

    def extra_repr(self):
        return &#39;method={}, alpha={}&#39;.format(
            self.method, self.alpha
        )

&#34;&#34;&#34; Check gradients
from torch.autograd import gradcheck

alpha = 1.0
# alpha = 0.2
# alpha = 5.0

method = Quadratic
# method = PseudoHuber
# method = Huber
# method = Welsch # Can fail gradcheck due to numerically-necessary gradient clipping
# method = TruncatedQuadratic

robustPool = RobustGlobalPool2dFn.apply
alpha_tensor = torch.tensor([alpha], dtype=torch.double, requires_grad=False)
input = (torch.randn(2, 3, 7, 7, dtype=torch.double, requires_grad=True), method, alpha_tensor)
test = gradcheck(robustPool, input, eps=1e-6, atol=1e-4, rtol=1e-3, raise_exception=True)
print(&#34;{}: {}&#34;.format(method.__name__, test))
&#34;&#34;&#34;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ddn.pytorch.robustpool.Huber"><code class="flex name class">
<span>class <span class="ident">Huber</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Huber():
    is_convex = True

    @staticmethod
    def phi(z, alpha = 1.0):
        &#34;&#34;&#34; Huber penalty function

                        | 0.5 z^2 for |z| &lt;= alpha
        phi(z; alpha) = |
                        | alpha (|z| - 0.5 alpha) else

        Arguments:
            z: (b, ...) Torch tensor,
                batch of residuals

            alpha: float, optional, default: 1.0,
                slope of the linear region
                maximum residual in the quadratic region

        Return Values:
            phi_z: (b, ...) Torch tensor,
                Huber penalty associated with each residual

        Complexity:
            O(1)
        &#34;&#34;&#34;
        assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
        z = z.abs()
        phi_at_z = torch.where(z &lt;= alpha, 0.5 * torch.pow(z, 2), alpha * (z - 0.5 * alpha))
        return phi_at_z

    @staticmethod
    def Dy(z, alpha = 1.0):
        # Derivative of y(x) for the Huber penalty function
        w = torch.where(z.abs() &lt;= alpha, torch.ones_like(z), torch.zeros_like(z))
        w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
        Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
        return Dy_at_x</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="ddn.pytorch.robustpool.Huber.is_convex"><code class="name">var <span class="ident">is_convex</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="ddn.pytorch.robustpool.Huber.Dy"><code class="name flex">
<span>def <span class="ident">Dy</span></span>(<span>z, alpha=1.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def Dy(z, alpha = 1.0):
    # Derivative of y(x) for the Huber penalty function
    w = torch.where(z.abs() &lt;= alpha, torch.ones_like(z), torch.zeros_like(z))
    w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
    Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
    return Dy_at_x</code></pre>
</details>
</dd>
<dt id="ddn.pytorch.robustpool.Huber.phi"><code class="name flex">
<span>def <span class="ident">phi</span></span>(<span>z, alpha=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Huber penalty function</p>
<pre><code>            | 0.5 z^2 for |z| &lt;= alpha
</code></pre>
<p>phi(z; alpha) = |
| alpha (|z| - 0.5 alpha) else</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>(b, &hellip;) Torch tensor,</code></dt>
<dd>batch of residuals</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, optional, default<code>: 1.0,</code></dt>
<dd>slope of the linear region
maximum residual in the quadratic region</dd>
</dl>
<p>Return Values:
phi_z: (b, &hellip;) Torch tensor,
Huber penalty associated with each residual</p>
<h2 id="complexity">Complexity</h2>
<p>O(1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def phi(z, alpha = 1.0):
    &#34;&#34;&#34; Huber penalty function

                    | 0.5 z^2 for |z| &lt;= alpha
    phi(z; alpha) = |
                    | alpha (|z| - 0.5 alpha) else

    Arguments:
        z: (b, ...) Torch tensor,
            batch of residuals

        alpha: float, optional, default: 1.0,
            slope of the linear region
            maximum residual in the quadratic region

    Return Values:
        phi_z: (b, ...) Torch tensor,
            Huber penalty associated with each residual

    Complexity:
        O(1)
    &#34;&#34;&#34;
    assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
    z = z.abs()
    phi_at_z = torch.where(z &lt;= alpha, 0.5 * torch.pow(z, 2), alpha * (z - 0.5 * alpha))
    return phi_at_z</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ddn.pytorch.robustpool.PseudoHuber"><code class="flex name class">
<span>class <span class="ident">PseudoHuber</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PseudoHuber():
    is_convex = True

    @staticmethod
    def phi(z, alpha = 1.0):
        &#34;&#34;&#34; Pseudo-Huber penalty function

        phi(z; alpha) = alpha^2 (sqrt{1 + (z / alpha)^2} - 1)

        Arguments:
            z: (b, ...) Torch tensor,
                batch of residuals

            alpha: float, optional, default: 1.0,
                ~slope of the linear region
                ~maximum residual in the quadratic region

        Return Values:
            phi_z: (b, ...) Torch tensor,
                Pseudo-Huber penalty associated with each residual

        Complexity:
            O(1)
        &#34;&#34;&#34;
        assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
        alpha2 = alpha * alpha
        phi_at_z = alpha2 * (torch.sqrt(1.0 + torch.pow(z, 2) / alpha2) - 1.0)
        return phi_at_z

    @staticmethod
    def Dy(z, alpha = 1.0):
        # Derivative of y(x) for the pseudo-Huber penalty function
        w = torch.pow(1.0 + torch.pow(z, 2) / (alpha * alpha), -1.5)
        w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
        Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
        return Dy_at_x</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="ddn.pytorch.robustpool.PseudoHuber.is_convex"><code class="name">var <span class="ident">is_convex</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="ddn.pytorch.robustpool.PseudoHuber.Dy"><code class="name flex">
<span>def <span class="ident">Dy</span></span>(<span>z, alpha=1.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def Dy(z, alpha = 1.0):
    # Derivative of y(x) for the pseudo-Huber penalty function
    w = torch.pow(1.0 + torch.pow(z, 2) / (alpha * alpha), -1.5)
    w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
    Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
    return Dy_at_x</code></pre>
</details>
</dd>
<dt id="ddn.pytorch.robustpool.PseudoHuber.phi"><code class="name flex">
<span>def <span class="ident">phi</span></span>(<span>z, alpha=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Pseudo-Huber penalty function</p>
<p>phi(z; alpha) = alpha^2 (sqrt{1 + (z / alpha)^2} - 1)</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>(b, &hellip;) Torch tensor,</code></dt>
<dd>batch of residuals</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, optional, default<code>: 1.0,</code></dt>
<dd>~slope of the linear region
~maximum residual in the quadratic region</dd>
</dl>
<p>Return Values:
phi_z: (b, &hellip;) Torch tensor,
Pseudo-Huber penalty associated with each residual</p>
<h2 id="complexity">Complexity</h2>
<p>O(1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def phi(z, alpha = 1.0):
    &#34;&#34;&#34; Pseudo-Huber penalty function

    phi(z; alpha) = alpha^2 (sqrt{1 + (z / alpha)^2} - 1)

    Arguments:
        z: (b, ...) Torch tensor,
            batch of residuals

        alpha: float, optional, default: 1.0,
            ~slope of the linear region
            ~maximum residual in the quadratic region

    Return Values:
        phi_z: (b, ...) Torch tensor,
            Pseudo-Huber penalty associated with each residual

    Complexity:
        O(1)
    &#34;&#34;&#34;
    assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
    alpha2 = alpha * alpha
    phi_at_z = alpha2 * (torch.sqrt(1.0 + torch.pow(z, 2) / alpha2) - 1.0)
    return phi_at_z</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ddn.pytorch.robustpool.Quadratic"><code class="flex name class">
<span>class <span class="ident">Quadratic</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Quadratic():
    is_convex = True

    @staticmethod
    def phi(z, alpha = 1.0):
        &#34;&#34;&#34; Quadratic penalty function

        phi(z; alpha) = 0.5 * z^2

        Arguments:
            z: (b, ...) Torch tensor,
                batch of residuals

            alpha: float, optional, default: 1.0,
                ignored

        Return Values:
            phi_z: (b, ...) Torch tensor,
                Quadratic penalty associated with each residual

        Complexity:
            O(1)
        &#34;&#34;&#34;
        phi_at_z = 0.5 * torch.pow(z, 2)
        return phi_at_z

    @staticmethod
    def Dy(z, alpha = 1.0):
        # Derivative of y(x) for the quadratic penalty function
        Dy_at_x = torch.ones_like(z) / (z.size(-1) * z.size(-2))
        return Dy_at_x</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="ddn.pytorch.robustpool.Quadratic.is_convex"><code class="name">var <span class="ident">is_convex</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="ddn.pytorch.robustpool.Quadratic.Dy"><code class="name flex">
<span>def <span class="ident">Dy</span></span>(<span>z, alpha=1.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def Dy(z, alpha = 1.0):
    # Derivative of y(x) for the quadratic penalty function
    Dy_at_x = torch.ones_like(z) / (z.size(-1) * z.size(-2))
    return Dy_at_x</code></pre>
</details>
</dd>
<dt id="ddn.pytorch.robustpool.Quadratic.phi"><code class="name flex">
<span>def <span class="ident">phi</span></span>(<span>z, alpha=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Quadratic penalty function</p>
<p>phi(z; alpha) = 0.5 * z^2</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>(b, &hellip;) Torch tensor,</code></dt>
<dd>batch of residuals</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, optional, default<code>: 1.0,</code></dt>
<dd>ignored</dd>
</dl>
<p>Return Values:
phi_z: (b, &hellip;) Torch tensor,
Quadratic penalty associated with each residual</p>
<h2 id="complexity">Complexity</h2>
<p>O(1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def phi(z, alpha = 1.0):
    &#34;&#34;&#34; Quadratic penalty function

    phi(z; alpha) = 0.5 * z^2

    Arguments:
        z: (b, ...) Torch tensor,
            batch of residuals

        alpha: float, optional, default: 1.0,
            ignored

    Return Values:
        phi_z: (b, ...) Torch tensor,
            Quadratic penalty associated with each residual

    Complexity:
        O(1)
    &#34;&#34;&#34;
    phi_at_z = 0.5 * torch.pow(z, 2)
    return phi_at_z</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ddn.pytorch.robustpool.RobustGlobalPool2d"><code class="flex name class">
<span>class <span class="ident">RobustGlobalPool2d</span></span>
<span>(</span><span>method, alpha=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RobustGlobalPool2d(torch.nn.Module):
    def __init__(self, method, alpha=1.0):
        super(RobustGlobalPool2d, self).__init__()
        self.method = method
        self.register_buffer(&#39;alpha&#39;, torch.tensor([alpha]))

    def forward(self, input):
        return RobustGlobalPool2dFn.apply(input,
                                          self.method,
                                          self.alpha
                                          )

    def extra_repr(self):
        return &#39;method={}, alpha={}&#39;.format(
            self.method, self.alpha
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ddn.pytorch.robustpool.RobustGlobalPool2d.extra_repr"><code class="name flex">
<span>def <span class="ident">extra_repr</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extra_repr(self):
    return &#39;method={}, alpha={}&#39;.format(
        self.method, self.alpha
    )</code></pre>
</details>
</dd>
<dt id="ddn.pytorch.robustpool.RobustGlobalPool2d.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input) -> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, input):
    return RobustGlobalPool2dFn.apply(input,
                                      self.method,
                                      self.alpha
                                      )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ddn.pytorch.robustpool.RobustGlobalPool2dFn"><code class="flex name class">
<span>class <span class="ident">RobustGlobalPool2dFn</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A function to globally pool a 2D response matrix using a robust penalty function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RobustGlobalPool2dFn(torch.autograd.Function):
    &#34;&#34;&#34;
    A function to globally pool a 2D response matrix using a robust penalty function
    &#34;&#34;&#34;
    @staticmethod
    def runOptimisation(x, y, method, alpha_scalar):
        with torch.enable_grad():
            opt = torch.optim.LBFGS([y],
                                    lr=1, # Default: 1
                                    max_iter=100, # Default: 20
                                    max_eval=None, # Default: None
                                    tolerance_grad=1e-05, # Default: 1e-05
                                    tolerance_change=1e-09, # Default: 1e-09
                                    history_size=100, # Default: 100
                                    line_search_fn=None # Default: None, Alternative: &#34;strong_wolfe&#34;
                                    )
            def reevaluate():
                opt.zero_grad()
                # Sum cost function across residuals and batch (all fi are positive)
                f = method.phi(y.unsqueeze(-1).unsqueeze(-1) - x, alpha=alpha_scalar).sum()
                f.backward()
                return f
            opt.step(reevaluate)
        return y

    @staticmethod
    def forward(ctx, x, method, alpha):
        input_size = x.size()
        assert len(input_size) &gt;= 2, &#34;input must at least 2D (%d &lt; 2)&#34; % len(input_size)
        alpha_scalar = alpha.item()
        assert alpha.item() &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha.item()
        x = x.detach()
        x = x.flatten(end_dim=-3) if len(input_size) &gt; 2 else x
        # Handle non-convex functions separately
        if method.is_convex:
            # Use mean as initial guess
            y = x.mean([-2, -1]).clone().requires_grad_()
            y = RobustGlobalPool2dFn.runOptimisation(x, y, method, alpha_scalar)
        else:
            # Use mean and median as initial guesses and choose the best
            # ToDo: multiple random starts
            y_mean = x.mean([-2, -1]).clone().requires_grad_()
            y_mean = RobustGlobalPool2dFn.runOptimisation(x, y_mean, method, alpha_scalar)
            y_median = x.flatten(start_dim=-2).median(dim=-1)[0].clone().requires_grad_()
            y_median = RobustGlobalPool2dFn.runOptimisation(x, y_median, method, alpha_scalar)
            f_mean = method.phi(y_mean.unsqueeze(-1).unsqueeze(-1) - x, alpha=alpha_scalar).sum(-1).sum(-1)
            f_median = method.phi(y_median.unsqueeze(-1).unsqueeze(-1) - x, alpha=alpha_scalar).sum(-1).sum(-1)
            y = torch.where(f_mean &lt;= f_median, y_mean, y_median)
        y = y.detach()
        z = (y.unsqueeze(-1).unsqueeze(-1) - x).clone()
        ctx.method = method
        ctx.input_size = input_size
        ctx.save_for_backward(z, alpha)
        return y.reshape(input_size[:-2]).clone()

    @staticmethod
    def backward(ctx, grad_output):
        z, alpha = ctx.saved_tensors
        input_size = ctx.input_size
        method = ctx.method
        grad_input = None
        if ctx.needs_input_grad[0]:
            # Flatten:
            grad_output = grad_output.detach().flatten(end_dim=-1)
            # Use implicit differentiation to compute derivative:
            grad_input = method.Dy(z, alpha) * grad_output.unsqueeze(-1).unsqueeze(-1)
            # Unflatten:
            grad_input = grad_input.reshape(input_size)
        return grad_input, None, None</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.autograd.function.Function</li>
<li>torch.autograd.function._SingleLevelFunction</li>
<li>torch._C._FunctionBase</li>
<li>torch.autograd.function.FunctionCtx</li>
<li>torch.autograd.function._HookMixin</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="ddn.pytorch.robustpool.RobustGlobalPool2dFn.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>ctx, grad_output)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines a formula for differentiating the operation with backward mode
automatic differentiation (alias to the vjp function).</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context :attr:<code>ctx</code> as the first argument, followed by
as many outputs as the :func:<code>forward</code> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
:func:<code>forward</code>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute :attr:<code>ctx.needs_input_grad</code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
:func:<code>backward</code> will have <code>ctx.needs_input_grad[0] = True</code> if the
first input to :func:<code>forward</code> needs gradient computed w.r.t. the
output.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def backward(ctx, grad_output):
    z, alpha = ctx.saved_tensors
    input_size = ctx.input_size
    method = ctx.method
    grad_input = None
    if ctx.needs_input_grad[0]:
        # Flatten:
        grad_output = grad_output.detach().flatten(end_dim=-1)
        # Use implicit differentiation to compute derivative:
        grad_input = method.Dy(z, alpha) * grad_output.unsqueeze(-1).unsqueeze(-1)
        # Unflatten:
        grad_input = grad_input.reshape(input_size)
    return grad_input, None, None</code></pre>
</details>
</dd>
<dt id="ddn.pytorch.robustpool.RobustGlobalPool2dFn.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>ctx, x, method, alpha)</span>
</code></dt>
<dd>
<div class="desc"><p>This function is to be overridden by all subclasses. There are two ways
to define forward:</p>
<p>Usage 1 (Combined forward and ctx)::</p>
<pre><code>@staticmethod
def forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:
    pass
</code></pre>
<ul>
<li>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</li>
<li>See :ref:<code>combining-forward-context</code> for more details</li>
</ul>
<p>Usage 2 (Separate forward and ctx)::</p>
<pre><code>@staticmethod
def forward(*args: Any, **kwargs: Any) -&gt; Any:
    pass

@staticmethod
def setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:
    pass
</code></pre>
<ul>
<li>The forward no longer accepts a ctx argument.</li>
<li>Instead, you must also override the :meth:<code>torch.autograd.Function.setup_context</code>
staticmethod to handle setting up the <code>ctx</code> object.
<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs
to the forward.</li>
<li>See :ref:<code>extending-autograd</code> for more details</li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <code>ctx</code> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
:func:<code>ctx.save_for_backward</code> if they are intended to be used in
<code>backward</code> (equivalently, <code>vjp</code>) or :func:<code>ctx.save_for_forward</code>
if they are intended to be used for in <code>jvp</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def forward(ctx, x, method, alpha):
    input_size = x.size()
    assert len(input_size) &gt;= 2, &#34;input must at least 2D (%d &lt; 2)&#34; % len(input_size)
    alpha_scalar = alpha.item()
    assert alpha.item() &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha.item()
    x = x.detach()
    x = x.flatten(end_dim=-3) if len(input_size) &gt; 2 else x
    # Handle non-convex functions separately
    if method.is_convex:
        # Use mean as initial guess
        y = x.mean([-2, -1]).clone().requires_grad_()
        y = RobustGlobalPool2dFn.runOptimisation(x, y, method, alpha_scalar)
    else:
        # Use mean and median as initial guesses and choose the best
        # ToDo: multiple random starts
        y_mean = x.mean([-2, -1]).clone().requires_grad_()
        y_mean = RobustGlobalPool2dFn.runOptimisation(x, y_mean, method, alpha_scalar)
        y_median = x.flatten(start_dim=-2).median(dim=-1)[0].clone().requires_grad_()
        y_median = RobustGlobalPool2dFn.runOptimisation(x, y_median, method, alpha_scalar)
        f_mean = method.phi(y_mean.unsqueeze(-1).unsqueeze(-1) - x, alpha=alpha_scalar).sum(-1).sum(-1)
        f_median = method.phi(y_median.unsqueeze(-1).unsqueeze(-1) - x, alpha=alpha_scalar).sum(-1).sum(-1)
        y = torch.where(f_mean &lt;= f_median, y_mean, y_median)
    y = y.detach()
    z = (y.unsqueeze(-1).unsqueeze(-1) - x).clone()
    ctx.method = method
    ctx.input_size = input_size
    ctx.save_for_backward(z, alpha)
    return y.reshape(input_size[:-2]).clone()</code></pre>
</details>
</dd>
<dt id="ddn.pytorch.robustpool.RobustGlobalPool2dFn.runOptimisation"><code class="name flex">
<span>def <span class="ident">runOptimisation</span></span>(<span>x, y, method, alpha_scalar)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def runOptimisation(x, y, method, alpha_scalar):
    with torch.enable_grad():
        opt = torch.optim.LBFGS([y],
                                lr=1, # Default: 1
                                max_iter=100, # Default: 20
                                max_eval=None, # Default: None
                                tolerance_grad=1e-05, # Default: 1e-05
                                tolerance_change=1e-09, # Default: 1e-09
                                history_size=100, # Default: 100
                                line_search_fn=None # Default: None, Alternative: &#34;strong_wolfe&#34;
                                )
        def reevaluate():
            opt.zero_grad()
            # Sum cost function across residuals and batch (all fi are positive)
            f = method.phi(y.unsqueeze(-1).unsqueeze(-1) - x, alpha=alpha_scalar).sum()
            f.backward()
            return f
        opt.step(reevaluate)
    return y</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ddn.pytorch.robustpool.TruncatedQuadratic"><code class="flex name class">
<span>class <span class="ident">TruncatedQuadratic</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TruncatedQuadratic():
    is_convex = False

    @staticmethod
    def phi(z, alpha = 1.0):
        &#34;&#34;&#34; Truncated quadratic penalty function

                        | 0.5 z^2 for |z| &lt;= alpha
        phi(z; alpha) = |
                        | 0.5 alpha^2 else

        Arguments:
            z: (b, ...) Torch tensor,
                batch of residuals

            alpha: float, optional, default: 1.0,
                maximum residual in the quadratic region

        Return Values:
            phi_z: (b, ...) Torch tensor,
                Truncated quadratic penalty associated with each residual

        Complexity:
            O(1)
        &#34;&#34;&#34;
        assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
        z = z.abs()
        phi_at_z = torch.where(z &lt;= alpha, 0.5 * torch.pow(z, 2), 0.5 * alpha * alpha * torch.ones_like(z))
        return phi_at_z

    @staticmethod
    def Dy(z, alpha = 1.0):
        # Derivative of y(x) for the truncated quadratic penalty function
        w = torch.where(z.abs() &lt;= alpha, torch.ones_like(z), torch.zeros_like(z))
        w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
        Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
        return Dy_at_x</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="ddn.pytorch.robustpool.TruncatedQuadratic.is_convex"><code class="name">var <span class="ident">is_convex</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="ddn.pytorch.robustpool.TruncatedQuadratic.Dy"><code class="name flex">
<span>def <span class="ident">Dy</span></span>(<span>z, alpha=1.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def Dy(z, alpha = 1.0):
    # Derivative of y(x) for the truncated quadratic penalty function
    w = torch.where(z.abs() &lt;= alpha, torch.ones_like(z), torch.zeros_like(z))
    w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
    Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
    return Dy_at_x</code></pre>
</details>
</dd>
<dt id="ddn.pytorch.robustpool.TruncatedQuadratic.phi"><code class="name flex">
<span>def <span class="ident">phi</span></span>(<span>z, alpha=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Truncated quadratic penalty function</p>
<pre><code>            | 0.5 z^2 for |z| &lt;= alpha
</code></pre>
<p>phi(z; alpha) = |
| 0.5 alpha^2 else</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>(b, &hellip;) Torch tensor,</code></dt>
<dd>batch of residuals</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, optional, default<code>: 1.0,</code></dt>
<dd>maximum residual in the quadratic region</dd>
</dl>
<p>Return Values:
phi_z: (b, &hellip;) Torch tensor,
Truncated quadratic penalty associated with each residual</p>
<h2 id="complexity">Complexity</h2>
<p>O(1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def phi(z, alpha = 1.0):
    &#34;&#34;&#34; Truncated quadratic penalty function

                    | 0.5 z^2 for |z| &lt;= alpha
    phi(z; alpha) = |
                    | 0.5 alpha^2 else

    Arguments:
        z: (b, ...) Torch tensor,
            batch of residuals

        alpha: float, optional, default: 1.0,
            maximum residual in the quadratic region

    Return Values:
        phi_z: (b, ...) Torch tensor,
            Truncated quadratic penalty associated with each residual

    Complexity:
        O(1)
    &#34;&#34;&#34;
    assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
    z = z.abs()
    phi_at_z = torch.where(z &lt;= alpha, 0.5 * torch.pow(z, 2), 0.5 * alpha * alpha * torch.ones_like(z))
    return phi_at_z</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ddn.pytorch.robustpool.Welsch"><code class="flex name class">
<span>class <span class="ident">Welsch</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Welsch():
    is_convex = False

    @staticmethod
    def phi(z, alpha = 1.0):
        &#34;&#34;&#34; Welsch penalty function

        phi(z; alpha) = 1 - exp(-0.5 * z^2 / alpha^2)

        Arguments:
            z: (b, ...) Torch tensor,
                batch of residuals

            alpha: float, optional, default: 1.0,
                ~maximum residual in the quadratic region

        Return Values:
            phi_z: (b, ...) Torch tensor,
                Welsch penalty associated with each residual

        Complexity:
            O(1)
        &#34;&#34;&#34;
        assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
        phi_at_z = 1.0 - torch.exp(-torch.pow(z, 2) / (2.0 * alpha * alpha))
        return phi_at_z

    @staticmethod
    def Dy(z, alpha = 1.0):
        # Derivative of y(x) for the Welsch penalty function
        alpha2 = alpha * alpha
        z2_on_alpha2 = torch.pow(z, 2) / alpha2
        w = (1.0 - z2_on_alpha2) * torch.exp(-0.5 * z2_on_alpha2) / alpha2
        w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
        Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
        Dy_at_x = torch.clamp(Dy_at_x, -1.0, 1.0) # Clip gradients to +/- 1
        return Dy_at_x</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="ddn.pytorch.robustpool.Welsch.is_convex"><code class="name">var <span class="ident">is_convex</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="ddn.pytorch.robustpool.Welsch.Dy"><code class="name flex">
<span>def <span class="ident">Dy</span></span>(<span>z, alpha=1.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def Dy(z, alpha = 1.0):
    # Derivative of y(x) for the Welsch penalty function
    alpha2 = alpha * alpha
    z2_on_alpha2 = torch.pow(z, 2) / alpha2
    w = (1.0 - z2_on_alpha2) * torch.exp(-0.5 * z2_on_alpha2) / alpha2
    w_sum = w.sum(dim=-1, keepdim=True).sum(dim=-2, keepdim=True).expand_as(w)
    Dy_at_x = torch.where(w_sum.abs() &lt;= 1e-9, torch.zeros_like(w), w.div(w_sum))
    Dy_at_x = torch.clamp(Dy_at_x, -1.0, 1.0) # Clip gradients to +/- 1
    return Dy_at_x</code></pre>
</details>
</dd>
<dt id="ddn.pytorch.robustpool.Welsch.phi"><code class="name flex">
<span>def <span class="ident">phi</span></span>(<span>z, alpha=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Welsch penalty function</p>
<p>phi(z; alpha) = 1 - exp(-0.5 * z^2 / alpha^2)</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>(b, &hellip;) Torch tensor,</code></dt>
<dd>batch of residuals</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, optional, default<code>: 1.0,</code></dt>
<dd>~maximum residual in the quadratic region</dd>
</dl>
<p>Return Values:
phi_z: (b, &hellip;) Torch tensor,
Welsch penalty associated with each residual</p>
<h2 id="complexity">Complexity</h2>
<p>O(1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def phi(z, alpha = 1.0):
    &#34;&#34;&#34; Welsch penalty function

    phi(z; alpha) = 1 - exp(-0.5 * z^2 / alpha^2)

    Arguments:
        z: (b, ...) Torch tensor,
            batch of residuals

        alpha: float, optional, default: 1.0,
            ~maximum residual in the quadratic region

    Return Values:
        phi_z: (b, ...) Torch tensor,
            Welsch penalty associated with each residual

    Complexity:
        O(1)
    &#34;&#34;&#34;
    assert alpha &gt; 0.0, &#34;alpha must be strictly positive (%f &lt;= 0)&#34; % alpha
    phi_at_z = 1.0 - torch.exp(-torch.pow(z, 2) / (2.0 * alpha * alpha))
    return phi_at_z</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ddn.pytorch" href="index.html">ddn.pytorch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ddn.pytorch.robustpool.Huber" href="#ddn.pytorch.robustpool.Huber">Huber</a></code></h4>
<ul class="">
<li><code><a title="ddn.pytorch.robustpool.Huber.Dy" href="#ddn.pytorch.robustpool.Huber.Dy">Dy</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.Huber.is_convex" href="#ddn.pytorch.robustpool.Huber.is_convex">is_convex</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.Huber.phi" href="#ddn.pytorch.robustpool.Huber.phi">phi</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ddn.pytorch.robustpool.PseudoHuber" href="#ddn.pytorch.robustpool.PseudoHuber">PseudoHuber</a></code></h4>
<ul class="">
<li><code><a title="ddn.pytorch.robustpool.PseudoHuber.Dy" href="#ddn.pytorch.robustpool.PseudoHuber.Dy">Dy</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.PseudoHuber.is_convex" href="#ddn.pytorch.robustpool.PseudoHuber.is_convex">is_convex</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.PseudoHuber.phi" href="#ddn.pytorch.robustpool.PseudoHuber.phi">phi</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ddn.pytorch.robustpool.Quadratic" href="#ddn.pytorch.robustpool.Quadratic">Quadratic</a></code></h4>
<ul class="">
<li><code><a title="ddn.pytorch.robustpool.Quadratic.Dy" href="#ddn.pytorch.robustpool.Quadratic.Dy">Dy</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.Quadratic.is_convex" href="#ddn.pytorch.robustpool.Quadratic.is_convex">is_convex</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.Quadratic.phi" href="#ddn.pytorch.robustpool.Quadratic.phi">phi</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ddn.pytorch.robustpool.RobustGlobalPool2d" href="#ddn.pytorch.robustpool.RobustGlobalPool2d">RobustGlobalPool2d</a></code></h4>
<ul class="">
<li><code><a title="ddn.pytorch.robustpool.RobustGlobalPool2d.extra_repr" href="#ddn.pytorch.robustpool.RobustGlobalPool2d.extra_repr">extra_repr</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.RobustGlobalPool2d.forward" href="#ddn.pytorch.robustpool.RobustGlobalPool2d.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ddn.pytorch.robustpool.RobustGlobalPool2dFn" href="#ddn.pytorch.robustpool.RobustGlobalPool2dFn">RobustGlobalPool2dFn</a></code></h4>
<ul class="">
<li><code><a title="ddn.pytorch.robustpool.RobustGlobalPool2dFn.backward" href="#ddn.pytorch.robustpool.RobustGlobalPool2dFn.backward">backward</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.RobustGlobalPool2dFn.forward" href="#ddn.pytorch.robustpool.RobustGlobalPool2dFn.forward">forward</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.RobustGlobalPool2dFn.runOptimisation" href="#ddn.pytorch.robustpool.RobustGlobalPool2dFn.runOptimisation">runOptimisation</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ddn.pytorch.robustpool.TruncatedQuadratic" href="#ddn.pytorch.robustpool.TruncatedQuadratic">TruncatedQuadratic</a></code></h4>
<ul class="">
<li><code><a title="ddn.pytorch.robustpool.TruncatedQuadratic.Dy" href="#ddn.pytorch.robustpool.TruncatedQuadratic.Dy">Dy</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.TruncatedQuadratic.is_convex" href="#ddn.pytorch.robustpool.TruncatedQuadratic.is_convex">is_convex</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.TruncatedQuadratic.phi" href="#ddn.pytorch.robustpool.TruncatedQuadratic.phi">phi</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ddn.pytorch.robustpool.Welsch" href="#ddn.pytorch.robustpool.Welsch">Welsch</a></code></h4>
<ul class="">
<li><code><a title="ddn.pytorch.robustpool.Welsch.Dy" href="#ddn.pytorch.robustpool.Welsch.Dy">Dy</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.Welsch.is_convex" href="#ddn.pytorch.robustpool.Welsch.is_convex">is_convex</a></code></li>
<li><code><a title="ddn.pytorch.robustpool.Welsch.phi" href="#ddn.pytorch.robustpool.Welsch.phi">phi</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>