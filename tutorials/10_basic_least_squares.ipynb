{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61b67a1",
   "metadata": {},
   "source": [
    "# Basic Differentiable Least Squares\n",
    "\n",
    "In this tutorial we develop a declarative node in PyTorch for solving the ordinary least squares problem,\n",
    "\n",
    "$$\n",
    "    \\text{minimize} \\quad \\|Ax - b\\|_2^2\n",
    "$$\n",
    "\n",
    "parameterized by $m$-by-$n$ matrix $A$ and $m$-vector $b$. The output has closed-form solution\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    x^\\star &= \\left(A^T \\! A\\right)^{-1} \\! A^T b \\\\\n",
    "    &= R^{-1} Q^{T} b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $A = QR$ is the QR decomposition of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aafed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T00:01:17.291352Z",
     "start_time": "2022-12-07T00:01:16.004171Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.get_device_name() if torch.cuda.is_available() else \"No CUDA\")\n",
    "\n",
    "torch.manual_seed(22)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344e9e7",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "Since QR decomposition is differentiable we can simply produce a PyTorch function for solving the least-squares problem in the forward pass and allow `autograd` to automatically compute gradients in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74030c32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T00:01:17.306567Z",
     "start_time": "2022-12-07T00:01:17.293347Z"
    }
   },
   "outputs": [],
   "source": [
    "def solve_by_qr(A, b):\n",
    "    \"\"\"Auto-differentiable solver for least squares by QR.\"\"\"\n",
    "    B, M, N = A.shape\n",
    "    assert b.shape == (B, M, 1)\n",
    "\n",
    "    Q, R = torch.linalg.qr(A, mode='reduced')\n",
    "    return torch.linalg.solve_triangular(R, torch.bmm(b.view(B, 1, M), Q).view(B, N, 1), upper=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187cfd3",
   "metadata": {},
   "source": [
    "Indeed the PyTorch `torch.linalg.lstsq` function is itself auto differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f933f11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T23:19:16.976852Z",
     "start_time": "2022-12-06T23:19:16.959158Z"
    }
   },
   "source": [
    "### Custom Backward Pass\n",
    "\n",
    "We can develop a more efficient backward pass code by hand deriving the gradients.\n",
    "\n",
    "Consider differentiating $x^\\star$ with respect to single element $A_{ij}$, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\frac{\\text{d}}{\\text{d} A_{ij}} x^\\star \n",
    "\t&= \\frac{\\text{d}}{\\text{d} A_{ij}} \\left(A^T \\! A\\right)^{-1} \\! A^T b \\\\\n",
    "\t&= \\left(\\frac{\\text{d}}{\\text{d} A_{ij}} \\left(A^T \\! A\\right)^{-1} \\right) A^T b + \\left(A^T \\! A\\right)^{-1} \\! \\left(\\frac{\\text{d}}{\\text{d} A_{ij}}  A^T b\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now using the identity $\\frac{\\text{d}}{\\text{d} z} Z^{-1} = -Z^{-1} \\left(\\frac{\\text{d}}{\\text{d} z} Z\\right) Z^{-1}$ we get, for the first term,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\frac{\\text{d}}{\\text{d} A_{ij}} \\left(A^T \\! A\\right)^{-1} \n",
    "\t&= -\\left(A^T \\! A\\right)^{-1} \\!\\left(\\frac{\\text{d}}{\\text{d} A_{ij}} \\left(A^T \\! A\\right) \\right) \\left(A^T \\! A\\right)^{-1} \\\\\n",
    "\t&= -\\left(A^T \\! A\\right)^{-1} \\!\\left(E_{ij}^T A + A^T E_{ij}\\right) \\left(A^T \\! A\\right)^{-1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $E_{ij}$ is a matrix with one in the $(i,j)$-th element and zeros elsewhere. Furthermore, for the second term,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\frac{\\text{d}}{\\text{d} A_{ij}} A^T b &= E_{ij}^T b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Plugging these back into parent equation we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\frac{\\text{d}}{\\text{d} A_{ij}} x^\\star \n",
    "\t&= -\\left(A^T \\! A\\right)^{-1} \\!\\left(E_{ij}^T A + A^T E_{ij}\\right) \\left(A^T \\! A\\right)^{-1} \\! A^T b + \\left(A^T \\! A\\right)^{-1} E_{ij}^T b\n",
    "\t\\\\\n",
    "\t&= -\\left(A^T \\! A\\right)^{-1} \\!\\left(E_{ij}^T A + A^T E_{ij}\\right) x^\\star + \\left(A^T \\! A\\right)^{-1} E_{ij}^T b\n",
    "\t\\\\\n",
    "\t&= -\\left(A^T \\! A\\right)^{-1} \\left( E_{ij}^T (Ax^\\star - b) + A^T E_{ij} x^\\star \\right)\n",
    "\t\\\\\n",
    "\t&= -\\left(A^T \\! A\\right)^{-1} \\left( (a_i^T x^\\star - b_i) e_j + x_j^\\star a_i \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $e_j = (0, 0, \\ldots, 1, 0, \\ldots) \\in \\mathbb{R}^n$ is the $j$-th canonical vector, i.e., vector with a one in the $j$-th component and zeros everywhere else, and $a_i^T \\in \\mathbb{R}^{1 \\times n}$ is the $i$-th row of matrix $A$.\n",
    "\n",
    "Observe that the term $(A^TA)^{-1}$ appears both in the solution for $x$ and the derivatives with respect to each $A_{ij}$. Thus, we only need to factor $A$ once in the forward pass and cache $R$ for use during the backward pass. This saves significant compute. Moreover, we can reuse terms for different $\\frac{\\text{d}}{\\text{d} A_{ij}}$ and efficiently combine with the incoming gradient of the loss with respect to $x^\\star$ as we now show.\n",
    "\n",
    "Let $r = b - Ax^\\star$ and let $v^T$ denote the backward coming gradient $\\frac{\\text{d}}{\\text{d} x^\\star} L$. Then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\frac{\\text{d} L}{\\text{d} A_{ij}} &= v^T \\frac{\\text{d} x^\\star}{\\text{d} A_{ij}} \\\\\n",
    "\t&= v^T \\left(A^T \\! A\\right)^{-1} \\left( r_i e_j - x_j^\\star a_i \\right) \\\\\n",
    "\t&= w^T \\! \\left( r_i e_j - x_j^\\star a_i \\right) \\\\\n",
    "\t&= r_i w_j - w^T \\! a_i x_j^\\star\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $w = \\left(A^T \\! A\\right)^{-1}v$ does not depend on which $A_{ij}$ we are differentiating with respect to. We can therefore compute the entire matrix of $m \\times n$ derivatives efficiently as the sum of outer products\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\left(\\frac{\\text{d} L}{\\text{d} A}\\right)^{\\! T} = \\begin{bmatrix}\n",
    "\t\t\\displaystyle \\frac{\\text{d} L}{\\text{d} A_{ij}}\n",
    "\t\\end{bmatrix}_{\\substack{i=1, \\ldots, m \\\\ j=1, \\ldots, n}} &= rw^T - (Aw)(x^\\star)^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can also differentiate with respect to $b$, which is much simpler,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\left(\\frac{\\text{d} L}{\\text{d} b}\\right)^{\\! T} = Aw\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The following code implement a PyTorch function with hand-coded backward pass. It is also available as ``BasicLeastSquaresFcn`` in the ``ddn.pytorch.leastsquares`` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d0fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T00:01:17.321277Z",
     "start_time": "2022-12-07T00:01:17.310472Z"
    }
   },
   "outputs": [],
   "source": [
    "class BasicLeastSquaresFcn(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    PyTorch autograd function for basic least squares problems,\n",
    "\n",
    "        y = argmin_u \\|Au - b\\|^2\n",
    "\n",
    "    solved via QR decomposition.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, b, cache_decomposition=True):\n",
    "        B, M, N = A.shape\n",
    "        assert b.shape == (B, M, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Q, R = torch.linalg.qr(A, mode='reduced')\n",
    "            x = torch.linalg.solve_triangular(R, torch.bmm(b.view(B, 1, M), Q).view(B, N, 1), upper=True)\n",
    "\n",
    "        # save state for backward pass\n",
    "        ctx.save_for_backward(A, b, x, R if cache_decomposition else None)\n",
    "\n",
    "        # return solution\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dx):\n",
    "        # check for None tensors\n",
    "        if dx is None:\n",
    "            return None, None\n",
    "\n",
    "        # unpack cached tensors\n",
    "        A, b, x, R = ctx.saved_tensors\n",
    "        B, M, N = A.shape\n",
    "\n",
    "        if R is None:\n",
    "            Q, R = torch.linalg.qr(A, mode='r')\n",
    "\n",
    "        dA, db = None, None\n",
    "\n",
    "        w = torch.linalg.solve_triangular(R, torch.linalg.solve_triangular(torch.transpose(R, 2, 1), dx, upper=False), upper=True)\n",
    "        Aw = torch.bmm(A, w)\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            r = b - torch.bmm(A, x)\n",
    "            dA = torch.bmm(r.view(B, M, 1), w.view(B, 1, N)) - torch.bmm(Aw.view(B, M, 1), x.view(B, 1, N))\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            db = Aw\n",
    "\n",
    "        # return gradients (None for non-tensor inputs)\n",
    "        return dA, db, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec8c1c-c573-4740-b34c-1ea88bc2f3ec",
   "metadata": {},
   "source": [
    "We also demonstrate how to use automatic differentiation to compute some terms in the derivative expression (although not really necessary for this problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd6122-db74-49bf-89ca-c6301565e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicAutoDiffLeastSquaresFcn(BasicLeastSquaresFcn):\n",
    "    \"\"\"PyTorch autograd function for basic least squares problems using automatic differentiation.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dx):\n",
    "        # check for None tensors\n",
    "        if dx is None:\n",
    "            return None, None\n",
    "\n",
    "        # unpack cached tensors\n",
    "        A, b, x, R = ctx.saved_tensors\n",
    "        B, M, N = A.shape\n",
    "        \n",
    "        if R is None:\n",
    "            Q, R = torch.linalg.qr(A, mode='r')\n",
    "\n",
    "        # explicitly compute dLdg = v^T H^{-1}\n",
    "        w = torch.linalg.solve_triangular(R, torch.linalg.solve_triangular(torch.transpose(R, 2, 1), dx, upper=False), upper=True)\n",
    "        \n",
    "        # implicitly compute dLdA and dLdb from dLdg and g = (d/dx)(objective)\n",
    "        A_var = A.detach().requires_grad_(True)\n",
    "        b_var = b.detach().requires_grad_(True)\n",
    "        x_const = x.detach().requires_grad_(False)\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            g = torch.bmm(A_var.transpose(1, 2), b_var - torch.bmm(A_var, x_const))\n",
    "        dA, db = torch.autograd.grad(outputs=g, inputs=(A_var, b_var), grad_outputs=w, retain_graph=False)\n",
    "\t\t        \n",
    "        # return gradients (None for non-tensor inputs)\n",
    "        return dA, db, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1869b",
   "metadata": {},
   "source": [
    "### Profiling\n",
    "\n",
    "We profile the running time and memory required by the various implementations. Our test is to solve the bi-level optimisation problem,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\begin{array}{ll}\n",
    "\t\t\\text{minimize} & \\frac{1}{2} \\|x^\\star - x^{\\text{target}}\\|_2^2 \\\\\n",
    "\t\t\\text{subject to} & x^\\star = \\textrm{argmin}_{x} \\; \\|Ax - b\\|_2^2\n",
    "\t\\end{array}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "by gradient descent for a fixed number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45dbb3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T00:01:17.337230Z",
     "start_time": "2022-12-07T00:01:17.324267Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.profiler as profiler\n",
    "\n",
    "def do_profile(fcn, B, M, N, device=torch.device(\"cpu\"), seed=0, iters=1000, mem_iters=10):\n",
    "    \"\"\"\n",
    "    Profile implementation. Uses different number of itertions for memory profile\n",
    "    and speed profiling since memory profiling is slow.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    A_true = torch.randn((B, M, N), device=device, requires_grad=False)\n",
    "    b_true = torch.randn((B, M, 1), device=device, requires_grad=False)\n",
    "    x_true = torch.linalg.lstsq(A_true, b_true, driver='gels').solution\n",
    "    A_init = torch.randn((B, M, N), device=device, requires_grad=True)\n",
    "\n",
    "    # profile time\n",
    "    model = [torch.nn.Parameter(A_init.clone())]\n",
    "    optimizer = torch.optim.AdamW(model, lr=1.0e-3)\n",
    "    loss_trace = [None for i in range(iters)]\n",
    "\n",
    "    time_start = time.monotonic()\n",
    "    for i in range(len(loss_trace)):\n",
    "        x = fcn(model[0], b_true)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.mse_loss(x, x_true)\n",
    "        loss_trace[i] = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    duration = time.monotonic() - time_start\n",
    "\n",
    "    # profile memory\n",
    "    model = [torch.nn.Parameter(A_init.clone())]\n",
    "    optimizer = torch.optim.AdamW(model, lr=1.0e-3)\n",
    "\n",
    "    with profiler.profile(profile_memory=True) as prof:\n",
    "        for i in range(mem_iters):\n",
    "            x = fcn(model[0], b_true)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = torch.nn.functional.mse_loss(x, x_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if device == torch.device(\"cpu\"):\n",
    "        memory = prof.key_averages().total_average().cpu_memory_usage\n",
    "    else:\n",
    "        memory = prof.key_averages().total_average().cuda_memory_usage\n",
    "    prof.enabled = False\n",
    "    \n",
    "    return duration, memory, loss_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90960f8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T00:02:45.600798Z",
     "start_time": "2022-12-07T00:01:17.339227Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_profiling(device=torch.device(\"cpu\")):\n",
    "\n",
    "    # implementations to profile\n",
    "    if device == torch.device(\"cpu\"):\n",
    "        f_array = [\n",
    "            lambda A, b: torch.linalg.lstsq(A, b, driver='gels').solution,  # auto-diff torch least-squares solver\n",
    "            solve_by_qr,                                                    # auto-diff solve by QR decomposition\n",
    "            BasicLeastSquaresFcn.apply,                                     # custom diff cache QR decomposition\n",
    "            lambda A, b: BasicLeastSquaresFcn.apply(A, b, False),           # custom diff recompute QR decomposition\n",
    "            BasicAutoDiffLeastSquaresFcn.apply                              # custom diff using AD\n",
    "        ]\n",
    "        f_legend = ['torch.lstsq', 'autograd QR', 'custom QR', 'custom QR (no cache)', 'custom via AD']\n",
    "        styles = ['--', ':', '-', '-.', (0, (3, 5, 1, 5, 1, 5))]\n",
    "        colours = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#800080']\n",
    "        batch_size = 1\n",
    "        iters = 1000\n",
    "        \n",
    "    else:\n",
    "        f_array = [\n",
    "            solve_by_qr,                                                    # auto-diff solve by QR decomposition\n",
    "            BasicLeastSquaresFcn.apply,                                     # custom diff cache QR decomposition\n",
    "            lambda A, b: BasicLeastSquaresFcn.apply(A, b, False),           # custom diff recompute QR decomposition\n",
    "            BasicAutoDiffLeastSquaresFcn.apply                              # custom diff using AD            \n",
    "        ]\n",
    "        f_legend = ['autograd QR', 'custom QR', 'custom QR (no cache)', 'custom via AD']\n",
    "        styles = [':', '-', '-.', (0, (3, 5, 1, 5, 1, 5))]\n",
    "        colours = ['#ff7f0e', '#2ca02c', '#d62728', '#800080']\n",
    "        batch_size = 128\n",
    "        iters = 100\n",
    "        \n",
    "    runtime = [[] for i in range(len(f_array))]\n",
    "    memused = [[] for i in range(len(f_array))]\n",
    "    endloss = [[] for i in range(len(f_array))]\n",
    "\n",
    "    N_array = [100, 200, 300, 400, 500]\n",
    "    \n",
    "    for N in N_array:\n",
    "        print(\"...running on size {}-by-{}\".format(2*N, N))\n",
    "        for i in range(len(f_array)):\n",
    "            t, m, loss = do_profile(f_array[i], batch_size, 2 * N, N, device=device, iters=iters)\n",
    "            runtime[i].append(t)\n",
    "            memused[i].append(m)\n",
    "            endloss[i].append(loss[-1])\n",
    "\n",
    "    for i in range(len(f_array)):\n",
    "        print(\"time: {}\\n mem: {}\\nloss: {}\".format(runtime[i], memused[i], endloss[i]))\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(len(f_array)):\n",
    "        plt.plot(N_array, runtime[i], linestyle=styles[i], color=colours[i])\n",
    "    plt.xlabel('problem size (2n-by-n)', fontsize=12)\n",
    "    plt.ylabel('running time (seconds)', fontsize=12)\n",
    "    plt.title('Running time on {} with batch size {}'.format(device, batch_size))\n",
    "    plt.legend(f_legend, fontsize=12)\n",
    "    #plt.savefig(\"ls_runtime_{}.png\".format(device), dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(len(f_array)):\n",
    "        plt.plot(N_array, [m  / (1024 * 1024) for m in memused[i]], linestyle=styles[i], color=colours[i])\n",
    "    plt.xlabel('problem size (2n-by-n)', fontsize=12)\n",
    "    plt.ylabel('memory used (MB)', fontsize=12)\n",
    "    plt.title('Memory usage on {} with batch size {}'.format(device, batch_size))\n",
    "    #plt.savefig(\"ls_memory_{}.png\".format(device), dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514126b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile on cpu\n",
    "plot_profiling(torch.device(\"cpu\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile on gpu\n",
    "if torch.cuda.is_available():\n",
    "    plot_profiling(torch.device(\"cuda\"))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
